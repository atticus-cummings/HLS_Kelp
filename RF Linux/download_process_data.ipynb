{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atticus/miniconda3/envs/hls-linux/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from contextlib import redirect_stdout\n",
    "import csv\n",
    "import earthaccess\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import binary_dilation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from rasterio.errors import RasterioIOError\n",
    "import csv\n",
    "from skimage import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cudf\n",
    "import cuml\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import shutil\n",
    "import pickle\n",
    "import cupy as cp\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x7f476b3dffd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_path = '/mnt/c/Users/attic/HLS_Kelp/maps/Isla_Vista_Kelp.geojson'\n",
    "\n",
    "load_num = -1 #sets number of granules to load, this should generally be >> than num_download;  -1 loads all \n",
    "specific_tile = True #set true if you only want specific tile \n",
    "retrain = False\n",
    "reclassify = True #Reclassify previously classified images\n",
    "show_image = True\n",
    "save_final_data = True\n",
    "save_to_path = '/mnt/c/Users/attic/HLS_Kelp/imagery/' # + tile_processed\n",
    "num_classify =150\n",
    "\n",
    "tile = '11SKU'\n",
    "location = 'Isla_Vista_Kelp'\n",
    "cloud_cover_threshold = .4\n",
    "version = 1\n",
    "temporal = (\"2019-8-01T00:00:00\", \"2020-1-01T00:00:00\") #\n",
    "dem_path = '/mnt/c/Users/attic/HLS_Kelp/imagery/Socal_DEM.tiff'\n",
    "rf_path = r'/mnt/c/users/attic/hls_kelp/random_forest/cu_rf3'\n",
    "sample_geotiff_path = '/mnt/c/Users/attic/HLS_Kelp/imagery/Isla_Vista_Kelp/11SKU/HLS.L30.T11SKU.2018234T183336.v2.0/HLS.L30.T11SKU.2018234T183336.v2.0.B03.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = gp.read_file(geojson_path)\n",
    "bbox = tuple(list(field.total_bounds))\n",
    "bbox #Display coordinate bounds\n",
    "with open(geojson_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Extract the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 75\n"
     ]
    }
   ],
   "source": [
    "#Search for satellite data from  Landsat 30m and Sentinel 30m\n",
    "results = earthaccess.search_data(\n",
    "    short_name=['HLSL30','HLSS30'],\n",
    "    bounding_box=bbox,\n",
    "    temporal=temporal,\n",
    "     cloud_cover=0, #Determine cloud cover\n",
    "    count=load_num\n",
    ")\n",
    "\n",
    "#print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 18/18 [00:00<00:00, 3103.32it/s]\n",
      "PROCESSING TASKS | :   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bathymetry and landmask built. Beginning individual granule processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING TASKS | :   6%|▌         | 1/18 [01:36<27:23, 96.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/pqdm/_base.py:65\u001b[0m, in \u001b[0;36m_parallel_process\u001b[0;34m(iterable, function, n_jobs, executor, argument_type, exception_behaviour, tqdm_class, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m processing_opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(futures)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm_class(as_completed(futures), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocessing_opts):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m## ======= download granule ======= ##\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f, redirect_stdout(f):\n\u001b[0;32m---> 82\u001b[0m         downloadPath \u001b[38;5;241m=\u001b[39m \u001b[43mearthaccess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgranule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Downloaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m### ====================  Begin processing each file  ====================  ####\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/earthaccess/api.py:188\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(granules, local_path, provider, threads)\u001b[0m\n\u001b[1;32m    186\u001b[0m     granules \u001b[38;5;241m=\u001b[39m [granules]\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mearthaccess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__store__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgranules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/earthaccess/store.py:475\u001b[0m, in \u001b[0;36mStore.get\u001b[0;34m(self, granules, local_path, provider, threads)\u001b[0m\n\u001b[1;32m    472\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m Path(local_path)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(granules):\n\u001b[0;32m--> 475\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgranules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/multimethod/__init__.py:375\u001b[0m, in \u001b[0;36mmultimethod.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DispatchError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/earthaccess/store.py:582\u001b[0m, in \u001b[0;36mStore._get_granules\u001b[0;34m(self, granules, local_path, provider, threads)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m downloaded_files\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# if the data are cloud-based, but we are not in AWS,\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# it will be downloaded as if it was on prem\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_onprem_granules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/earthaccess/store.py:643\u001b[0m, in \u001b[0;36mStore._download_onprem_granules\u001b[0;34m(self, urls, directory, threads)\u001b[0m\n\u001b[1;32m    640\u001b[0m directory\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    642\u001b[0m arguments \u001b[38;5;241m=\u001b[39m [(url, directory) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[0;32m--> 643\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43margument_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/pqdm/threads.py:22\u001b[0m, in \u001b[0;36mpqdm\u001b[0;34m(array, function, n_jobs, argument_type, bounded, exception_behaviour, tqdm_class, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpqdm\u001b[39m(\n\u001b[1;32m     13\u001b[0m     array: Iterable[Any],\n\u001b[1;32m     14\u001b[0m     function: Callable[[Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     21\u001b[0m ):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margument_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBoundedThreadPoolExecutor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbounded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_behaviour\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception_behaviour\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/site-packages/pqdm/_base.py:66\u001b[0m, in \u001b[0;36m_parallel_process\u001b[0;34m(iterable, function, n_jobs, executor, argument_type, exception_behaviour, tqdm_class, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     processing_opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(futures)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm_class(as_completed(futures), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocessing_opts):\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     68\u001b[0m collecting_opts \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(tqdm_opts)\n\u001b[1;32m     69\u001b[0m collecting_opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOLLECTING RESULTS | \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m collecting_opts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/concurrent/futures/_base.py:637\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/hls-linux/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(os.getcwd(),(f'/mnt/c/Users/attic/HLS_Kelp/imagery{location}'))\n",
    "temp_folder = os.path.join(os.path.join(folder_path),'temp')\n",
    "\n",
    "## ======= create location folder path ======= ##\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "if os.path.isdir(temp_folder):\n",
    "    shutil.rmtree(temp_folder)\n",
    "os.mkdir(temp_folder)\n",
    "iterations = 0\n",
    "\n",
    "### ==================== Load RF Classifier ================== ###\n",
    "\n",
    "with open(rf_path, 'rb') as f:\n",
    "    cu_rf = pickle.load(f)\n",
    "\n",
    "### ==================== Create DEM mask ====================  ####\n",
    "with rasterio.open(sample_geotiff_path) as dst:\n",
    "       \n",
    "    hls = dst.read()\n",
    "    dem = rasterio.open(dem_path)\n",
    "    if (dem.crs != dst.crs):\n",
    "        reprojected_dem = np.zeros((hls.shape[1], hls.shape[2]), dtype=hls.dtype)\n",
    "        reproject(\n",
    "            source=dem.read(),\n",
    "            destination=reprojected_dem,\n",
    "            src_transform=dem.transform,\n",
    "            src_crs=dem.crs,\n",
    "            dst_transform=dst.transform,\n",
    "            dst_crs=dst.crs,\n",
    "            resampling=Resampling.bilinear)\n",
    "    hls_flat = np.squeeze(hls, axis=0)   \n",
    "\n",
    "if not reprojected_dem.any():\n",
    "    print(\"Something failed, you better go check...\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    struct = np.ones((5,5))\n",
    "    land_mask = binary_dilation(reprojected_dem > 0, structure = struct)\n",
    "    ocean_mask = binary_dilation(reprojected_dem < -60 , structure = struct)\n",
    "    full_mask = land_mask + ocean_mask\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # plt.imshow(land_mask, cmap='gray')\n",
    "    # plt.show()    \n",
    "    \n",
    "print(\"Bathymetry and landmask built. Beginning individual granule processing\")\n",
    "\n",
    "for i, granule in enumerate(results):\n",
    "    if(iterations >num_classify):\n",
    "        break\n",
    "    if os.path.isdir(temp_folder):\n",
    "        shutil.rmtree(temp_folder)\n",
    "    os.mkdir(temp_folder)\n",
    "    ## ======= Parse metadata ======= ##\n",
    "    json_str = json.dumps(granule.__dict__)\n",
    "    metadata = json.loads(json_str) \n",
    "    meta = metadata['render_dict']['meta']\n",
    "    name = meta['native-id']\n",
    "\n",
    "    #For some reason, attributes are parsed into a list in the HLS metadata. This reformats it into a dictionary.\n",
    "    attributes_list = metadata['render_dict']['umm']['AdditionalAttributes']\n",
    "\n",
    "    attributes = {attr['Name']: attr['Values'][0] for attr in attributes_list}\n",
    "    #print(attributes['MGRS_TILE_ID'])\n",
    "    tile_type = attributes['MGRS_TILE_ID']\n",
    "    if(int(attributes['CLOUD_COVERAGE']) > 50): #Reject granules with large cloud cover, for now\n",
    "        print(\"Overall Cloud coverage >50%\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "    time = attributes['SENSING_TIME']\n",
    "    tile_folder = f\"{tile_type}_Classified_v{version}\"\n",
    "    if specific_tile and not tile_type == tile:\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "    ## ======= Create file directory, if needed  ======= ##\n",
    "    tile_path = os.path.join(folder_path,tile_folder)\n",
    "    if not os.path.isdir(tile_path):\n",
    "         os.mkdir(tile_path)\n",
    "\n",
    "    ## ======= download granule ======= ##\n",
    "    with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "        downloadPath = earthaccess.download(granule, temp_folder)\n",
    "    \n",
    "    print(f'{name} Downloaded')\n",
    "\n",
    "\n",
    "### ====================  Begin processing each file  ====================  ####\n",
    "\n",
    "    print(f\"Beginning {name} processing\")\n",
    "\n",
    "    ##==========Select Granule and Get File Names==========##\n",
    "\n",
    "    # Check sensor type and define bands for L8 and S2\n",
    "    \n",
    "    files = os.listdir(temp_folder)\n",
    "    file_data = name.split('.')\n",
    "    sensor = file_data[1]\n",
    "    if(sensor == 'L30'):\n",
    "        sensor_bands = ['B02','B03','B04','B05','B06','B07'] #2,3,4,5,6,7]\n",
    "    else:\n",
    "        sensor_bands = ['B02','B03','B04','B8A','B11','B12']\n",
    "    pattern = re.compile(r'\\.(' + '|'.join(sensor_bands) + r')\\.tif$')\n",
    "\n",
    "# Get File names\n",
    "    img_files = [f for f in files if re.search(pattern, f)]\n",
    "    f_mask = [f for f in files if re.search(r'Fmask\\.tif$', f)]\n",
    "    if not f_mask:\n",
    "        print(f\"Invalid granule: {name}\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "    if not len(img_files)  == 6:\n",
    "        print(f\"incomplete file download: {name}\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "\n",
    "    img_bands = []\n",
    "\n",
    "##==========Fmask Cloud mask==========##\n",
    "    #bitwise operations are weird. Far outside my comfort zone. Need to take CS33 first.........\n",
    "    try:\n",
    "        with rasterio.open(os.path.join(temp_folder,f_mask[0])) as fmask:\n",
    "            qa_band = fmask.read(1)\n",
    "        qa_bit = (1 << 1) - 1\n",
    "        qa_cloud_mask = ((qa_band >> 1) & qa_bit) == 1  # Bit 1 for cloud\n",
    "        qa_adjacent_to_cloud_mask = ((qa_band >> 2) & qa_bit) == 1  # Bit 2 for cloud adjacent\n",
    "        qa_cloud_shadow = ((qa_band >> 3) & qa_bit) == 1 \n",
    "        qa_ice = ((qa_band >> 4) & qa_bit) == 1 \n",
    "        #qa_water = ((qa_band >> 5) & qa_bit) == 1\n",
    "        qa_aerosol = (((qa_band >> 6) & 1) == 1) & (((qa_band >> 7) & 1) == 1)\n",
    "        cloud_mask = qa_cloud_mask | qa_cloud_shadow | qa_ice | qa_aerosol #Mask out Clouds and cloud-adjacent pixels \n",
    "        cloud_mask_2D = cloud_mask.reshape(-1).T\n",
    "    except RasterioIOError as e:\n",
    "        print(f\"Error reading file {file} in granule {name}: {e}\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue  # Skip to the next granule if a file cannot be read\n",
    "    #may not be necessary to mask out the cloud-adjacent pixels \n",
    "\n",
    "##========== Determine percentage of ocean covered by clouds ==========##\n",
    "    cloud_land_mask = cloud_mask | land_mask\n",
    "    cloud_but_not_land_mask = cloud_mask & ~land_mask\n",
    "    num_pixels_cloud_not_land = np.count_nonzero(cloud_but_not_land_mask)\n",
    "    num_pixels_not_land = np.count_nonzero(~land_mask)\n",
    "    percent_cloud_covered = num_pixels_cloud_not_land/num_pixels_not_land\n",
    "    if(percent_cloud_covered > cloud_cover_threshold):\n",
    "        print(f\"Percent clouds greater than threshold: {percent_cloud_covered}. Moving to next granule...\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "    print(f'{name} Percent cloud covered: {percent_cloud_covered}')\n",
    "    \n",
    " ##==========Create stacked np array, Apply landmask==========##\n",
    "    try:\n",
    "        for file in img_files:\n",
    "            with rasterio.open(os.path.join(temp_folder, file)) as src:\n",
    "                img_bands.append(np.where(cloud_land_mask, 0, src.read(1)))  # Create image with the various bands\n",
    "    except RasterioIOError as e:\n",
    "        print(f\"Error reading file {file} in granule {name}: {e}\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue  # Skip to the next granule if a file cannot be read\n",
    "    img = np.stack(img_bands, axis=0)\n",
    "    n_bands, height, width = img.shape\n",
    "    img_2D = img.reshape(img.shape[0], -1).T #classifier takes 2D array of band values for each pixel \n",
    "\n",
    " ##========== Normalize multi-spectral data ==========##\n",
    "\n",
    "    img_sum = img_2D.sum(axis=1)\n",
    "    epsilon = 1e-10  \n",
    "    img_2D_nor = np.divide(img_2D, img_sum[:, None] + epsilon, where=(img_sum[:, None] != 0))\n",
    "    img_2D_nor = (img_2D_nor * 255).astype(np.uint8)\n",
    "\n",
    " ##========== Classify Image with random forest ==========##\n",
    "    print(\"Beginning kelp classification\")\n",
    "    img_data = cudf.DataFrame(img_2D_nor)\n",
    "    img_data = img_data.astype(np.float32)\n",
    "\n",
    "    classification_pred = cu_rf.predict(img_data)\n",
    "    classified_img = classification_pred.values_host.reshape(width,height)\n",
    "\n",
    "    print(\"Classification finished\")\n",
    " ##========== Display image, if asked to ==========##\n",
    "    if show_image:\n",
    "        print(file)\n",
    "        plt.figure(figsize=(25, 25)) \n",
    "        plt.subplot(2, 1, 1)  \n",
    "        plt.imshow(classified_img)#[2700:3400, 600:2000])\n",
    "        plt.colorbar()\n",
    "        plt.title(file)\n",
    "        r_nor = img_2D_nor[:,2].reshape((height, width))\n",
    "        g_nor = img_2D_nor[:,1].reshape((height, width))\n",
    "        b_nor = img_2D_nor[:,0].reshape((height, width))\n",
    "        rgb_nor = np.stack([r_nor,g_nor,b_nor], axis=-1)  \n",
    "        rgb_cropped = rgb_nor#[2700:3400, 600:2000]\n",
    "        plt.subplot(2, 1, 2) \n",
    "        plt.imshow(rgb_cropped)\n",
    "        plt.title(\"RGB Cropped Image\")\n",
    "        #plt.colorbar()\n",
    "        plt.show()\n",
    " ##========== Prep for Mesma ==========##\n",
    "    ocean_dilation = np.ones((50,50)) #Struct for dilation (increase to enlarge non-ocean mask) larger --> takes longer\n",
    "    kelp_dilation = np.ones((20,20))\n",
    "    kelp_shrink = np.ones((1,1))\n",
    "    kelp_mask  = []\n",
    "    ocean_mask = []\n",
    "    print(\"Masking image for MESMA\")\n",
    "##========== Create mask for kelp and ocean ==========##\n",
    "    land_dilated = np.where(classified_img == 3, True, False)\n",
    "    clouds_dilated = np.where(classified_img == 2, True, False)\n",
    "    land_dilated = binary_dilation(land_dilated, structure=ocean_dilation)\n",
    "\n",
    "    ocean_dilated = land_dilated | clouds_dilated \n",
    "    #\n",
    "    kelp_dilated = np.where(classified_img == 0, True, False) #This is expanding hte kelp_mask so the TF is reversed\n",
    "    kelp_dilated = ~binary_dilation(~kelp_dilated, structure=kelp_shrink)\n",
    "    kelp_dilated = binary_dilation(kelp_dilated,structure=kelp_dilation) #I may not want to do this. we'll see\n",
    "    for i in range(4):\n",
    "        kmask = np.where(kelp_dilated == True, img[i],np.nan)\n",
    "        omask = np.where(ocean_dilated == False, img[i], np.nan)\n",
    "        kelp_mask.append(kmask)\n",
    "        ocean_mask.append(omask)\n",
    "\n",
    "    kelp_mask = np.array(kelp_mask)\n",
    "    ocean_mask = np.array(ocean_mask)\n",
    "\n",
    "    rgb_nor = np.stack([ocean_mask[2]/600,ocean_mask[0]/600,ocean_mask[1]/600], axis=-1)\n",
    "    rgb_nor_cropped = rgb_nor\n",
    "    #print(kelp_mask)\n",
    "    rgb_nor_cropped = np.ma.masked_where(np.isnan(rgb_nor_cropped), rgb_nor_cropped)\n",
    "    print(\"Generated final kelp & ocean mask\")\n",
    "##========== Display masked image, if asked to ==========##\n",
    "    if show_image:\n",
    "        display_image = kelp_mask[1]#,2500:3500,800:1800]\n",
    "        plt.figure(figsize=(30, 30), dpi=200)\n",
    "        plt.imshow(display_image, alpha=1)\n",
    "        plt.imshow(rgb_nor_cropped, alpha=1)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    " ##========== Prepare kelp and ocean endmembers ==========##\n",
    "    print(\"Gathering Endmembers\")\n",
    "    ocean_EM_stack = []\n",
    "    kelp_EM = [459, 556, 437, 1227]\n",
    "\n",
    "    n_bands, height, width = kelp_mask.shape\n",
    "    ocean_EM_n = 0\n",
    "    ocean_data = ocean_mask.reshape(ocean_mask.shape[0], -1)\n",
    "    kelp_data = kelp_mask.reshape(kelp_mask.shape[0],-1)\n",
    "\n",
    "    nan_columns = np.isnan(ocean_data).all(axis=0)  # Remove columns with nan \n",
    "\n",
    "\n",
    "\n",
    "    filtered_ocean = ocean_data[:, ~nan_columns]\n",
    "    if(len(filtered_ocean[0,:]) < 100):\n",
    "    \n",
    "        print(f\"Insufficient number of ocean pixels: {len(filtered_ocean[0,:])}\")\n",
    "        shutil.rmtree(temp_folder)\n",
    "        continue\n",
    "\n",
    "    for i in range(30):\n",
    "        index = random.randint(0,len(filtered_ocean[0])-1)\n",
    "        ocean_EM_stack.append(filtered_ocean[:,index])\n",
    "    ocean_EM = np.stack(ocean_EM_stack, axis=1)\n",
    "    #print(ocean_EM_array)\n",
    "\n",
    "    average_val = np.nanmean(filtered_ocean, axis=1)\n",
    "    average_endmember = np.nanmean(ocean_EM, axis=1)\n",
    "    print(f\"average EM Val: {average_endmember}\")\n",
    "    print(f\"average    Val: {average_val}\")\n",
    "\n",
    "   \n",
    "    kelp_mask = cp.asarray(kelp_mask)\n",
    "    ocean_EM = cp.asarray(ocean_EM)\n",
    "    kelp_EM = cp.asarray(kelp_EM)\n",
    "    kelp_data = cp.asarray(kelp_data)\n",
    "\n",
    "    frac1 = cp.full((kelp_mask.shape[1], kelp_mask.shape[2], 30), cp.nan)\n",
    "    frac2 = cp.full((kelp_mask.shape[1], kelp_mask.shape[2], 30), cp.nan)\n",
    "    rmse = cp.full((kelp_mask.shape[1], kelp_mask.shape[2], 30), cp.nan)\n",
    "    print(rmse.shape)\n",
    " ###========== Start MESMA ==========##\n",
    "    print(\"Running MESMA\")\n",
    "    for k in range(30):\n",
    "        B = cp.column_stack((ocean_EM[:, k], kelp_EM))\n",
    "        U, S, Vt = cp.linalg.svd(B, full_matrices=False)\n",
    "        IS = Vt.T / S\n",
    "        em_inv = IS @ U.T\n",
    "        F = em_inv @ kelp_data\n",
    "        model = (F.T @ B.T).T\n",
    "        resids = (kelp_data - model) / 10000\n",
    "        rmse[:, :, k] = cp.sqrt(cp.mean(resids**2, axis=0)).reshape(height, width)\n",
    "        frac1[:, :, k] = F[0, :].reshape(height, width)\n",
    "        frac2[:, :, k] = F[1, :].reshape(height, width)\n",
    "\n",
    "    print(\"MESMA Complete, building final image\")\n",
    "\n",
    "    minVals = cp.nanmin(rmse, axis=2)\n",
    "    PageIdx = cp.nanargmin(rmse, axis=2)\n",
    "    rows, cols = cp.meshgrid(cp.arange(rmse.shape[0]), cp.arange(rmse.shape[1]), indexing='ij')\n",
    "    Zindex = cp.ravel_multi_index((rows, cols, PageIdx), dims=rmse.shape)\n",
    "    Mes2 = frac2.ravel()[Zindex]\n",
    "    Mes2 = Mes2.T\n",
    "    Mes2 = -0.229 * Mes2**2 + 1.449 * Mes2 - 0.018 #Landsat mesma corrections \n",
    "    Mes2 = cp.clip(Mes2, 0, None)  # Ensure no negative values\n",
    "    Mes2 = cp.round(Mes2 * 100).astype(cp.int16)\n",
    "    if show_image:\n",
    "        kelp_img = cp.asnumpy(kelp_mask)\n",
    "        Mes_array = cp.asnumpy(Mes2).T\n",
    "        Mes_array_vis = np.where(Mes_array == 0, np.nan, Mes_array)\n",
    "        kelp_vis = np.where(kelp_img == 0, np.nan, kelp_img)\n",
    "        plt.figure(figsize=(20, 20), dpi=200)\n",
    "        plt.imshow(rgb_nor[1,2800:3100,800:1400])\n",
    "        plt.imshow(kelp_img[1,2800:3100,800:1400] , cmap='Greys', alpha=1)\n",
    "        plt.imshow(Mes_array_vis[2800:3100,800:1400], alpha=1)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    if save_final_data:\n",
    "        num_bands = 6\n",
    "        data_type = rasterio.int16\n",
    "        profile = {\n",
    "            'driver': 'GTiff',\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'count': 6,  # one band  B02, B03, B04, and B05, classified (Blue, Green, Red, and NIR).\n",
    "            'dtype': data_type,  # assuming binary mask, adjust dtype if needed\n",
    "            'crs': src.crs,\n",
    "            'transform': src.transform,\n",
    "            'nodata': 0  # assuming no data is 0\n",
    "        }\n",
    "        if not os.path.isdir(os.path.join(save_to_path,tile_type)):\n",
    "            os.mkdir(os.path.join(save_to_path,tile_type))\n",
    "        img_path = os.path.join(save_to_path,tile_type,f'{name}_processed.tif')\n",
    "\n",
    "        # Write the land mask array to GeoTIFF\n",
    "        with rasterio.open(img_path, 'w', **profile) as dst:\n",
    "                dst.write(img[0].astype(data_type), 1)\n",
    "                dst.write(img[1].astype(data_type), 2)\n",
    "                dst.write(img[2].astype(data_type), 3)\n",
    "                dst.write(img[3].astype(data_type), 4)\n",
    "                dst.write(classified_img.astype(data_type), 5)\n",
    "                dst.write(Mes_array.astype(data_type), 6)\n",
    "                \n",
    "        iterations = iterations + 1\n",
    "        print(f\"{iterations}/{len(results)}\")\n",
    "\n",
    "    print(f\"{name} Processing complete.\")\n",
    "    shutil.rmtree(temp_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls-linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
